{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6pK8bEqojUg"
      },
      "source": [
        "# Transfer Learning Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question\n",
        "- Your homework is to train on all digits\n",
        "- make your own handwritten data set of 5 characters (i.e. A, B, C, D, E) \n",
        "- transfer your minist trained model over to them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### make your own handwritten dataset of 5 characters (i.e. A, B, C, D, E). \n",
        "The English handwriting dataset will be used for this task:  \n",
        "https://www.kaggle.com/dhruvildave/english-handwritten-characters-dataset\n",
        "\n",
        "But this dataset contains handwritten numerical and all English alphabets. So we need to subest for the alphabets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F7iLjHRroY_r"
      },
      "outputs": [],
      "source": [
        "\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten,  Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "import PIL\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read in the new dataset. \n",
        "- It has a directory containing the handwritten alphabets.  \n",
        "- And a label's file.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {
        "id": "A75vVqYSXYh9"
      },
      "outputs": [],
      "source": [
        "path =r'.\\imgs_char_zipped\\imgs_char' \n",
        "#path = #'/content/drive/MyDrive/imgs_char/'\n",
        "label= r'.\\imgs_char_zipped\\english.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrcg7uBxzDBI",
        "outputId": "c7b31722-004b-4d54-ab04-933c799f38cb"
      },
      "outputs": [],
      "source": [
        "image_list=os.listdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "7ojCI6fz1IiB",
        "outputId": "2f51b925-e317-4ab5-e0da-1f5d25eb7daf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Img/img001-001.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Img/img001-002.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Img/img001-003.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Img/img001-004.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Img/img001-005.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3405</th>\n",
              "      <td>Img/img062-051.png</td>\n",
              "      <td>z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3406</th>\n",
              "      <td>Img/img062-052.png</td>\n",
              "      <td>z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3407</th>\n",
              "      <td>Img/img062-053.png</td>\n",
              "      <td>z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3408</th>\n",
              "      <td>Img/img062-054.png</td>\n",
              "      <td>z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3409</th>\n",
              "      <td>Img/img062-055.png</td>\n",
              "      <td>z</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3410 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   image label\n",
              "0     Img/img001-001.png     0\n",
              "1     Img/img001-002.png     0\n",
              "2     Img/img001-003.png     0\n",
              "3     Img/img001-004.png     0\n",
              "4     Img/img001-005.png     0\n",
              "...                  ...   ...\n",
              "3405  Img/img062-051.png     z\n",
              "3406  Img/img062-052.png     z\n",
              "3407  Img/img062-053.png     z\n",
              "3408  Img/img062-054.png     z\n",
              "3409  Img/img062-055.png     z\n",
              "\n",
              "[3410 rows x 2 columns]"
            ]
          },
          "execution_count": 372,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "#lbl_path= '/content/english.csv'\n",
        "labels1 = pd.read_csv(label)\n",
        "labels1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Match the label with the images list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting the filename from the 'image' array in the labels file.  \n",
        "- We split along the '/' delimiter and return the last segment that is the image file name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {
        "id": "y9dQexWR4gyV"
      },
      "outputs": [],
      "source": [
        "filename=[]\n",
        "for i in range(len(labels1)):\n",
        "  filename.append(labels1['image'][i].split('/')[-1])\n",
        "  \n",
        "labels1['filename']=filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {
        "id": "_TjdEs135zUW"
      },
      "outputs": [],
      "source": [
        "#Move the labels column to the last\n",
        "labels1['label_']=labels1['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "id": "A9b2q0tn6GA6"
      },
      "outputs": [],
      "source": [
        "#Drop redundant column\n",
        "labels1.drop(columns=['label'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_list = os.listdir(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling the images dataset.  \n",
        "- We use os.listdir() to access the list on images.\n",
        "- We use load_img to read in each image.  \n",
        "- Since the PIL function does not return the image as an array we will convert the returned PIL object to arrays of pixel values.\n",
        "- We will reshape the output to 28, 28 that we expect later.  \n",
        "- We put everything in a container.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvnevxPNnjHn",
        "outputId": "ebb7a3af-6964-4ab6-9468-009a6d6ada0a"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "import os\n",
        "\n",
        "img_arrays=[]\n",
        "image_lbl=[]\n",
        "#images_dataset={}\n",
        "\n",
        "for idx, i in enumerate(image_list):\n",
        "    #print(i)\n",
        "   #for j in range(len(labels1)):\n",
        "    if i == labels1['filename'][idx]:\n",
        "      real = os.path.join(path, i)\n",
        "      image = tf.keras.utils.load_img(path = real, color_mode=\"grayscale\", target_size=(28,28))\n",
        "      #img_array = tf.keras.utils.img_to_array(img=image, dtype=np.uint8)\n",
        "      img_array=tf.keras.preprocessing.image.img_to_array(img=image, dtype=np.uint8)\n",
        "      img_array=np.reshape(img_array, (28,28))\n",
        "      #print('idx==  ', labels1['filename'][idx], ' i== ', i)#, labels1['label'][idx])\n",
        "      img_arrays.append(img_array)\n",
        "      image_lbl.append(labels1['label_'][idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3410"
            ]
          },
          "execution_count": 378,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(image_lbl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Labels added to images.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIHbXekhwVr1",
        "outputId": "6cc1cb35-75be-4164-d4bc-691c7ec94a6e"
      },
      "outputs": [],
      "source": [
        "images_dataset = list(zip(image_lbl,img_arrays))\n",
        "#images_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {},
      "outputs": [],
      "source": [
        "#((images_dataset)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract only handwritten letters and exclude the handwritten digits.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "images_ds_ltr=[]\n",
        "#images_dict_ltr={}\n",
        "images_dict_ltr = collections.defaultdict(list)\n",
        "\n",
        "for k,v in images_dataset:\n",
        "    #print(v)\n",
        "    if k in ['A', 'B', 'C', 'D', 'E']:\n",
        "        #images_ds_ltr.append(images_dataset[i])\n",
        "        images_dict_ltr[k].append(v)\n",
        "print(len(images_dict_ltr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_ds = {}\n",
        "test_ltr=[]\n",
        "train_ltr=[]\n",
        "y_train3=[]\n",
        "y_test3=[]\n",
        "for k, v in images_dict_ltr.items():\n",
        "    X_train1, X_test1 = train_test_split(v, test_size=0.33, random_state=42)\n",
        "    df_ds[k]=(X_train1, X_test1) \n",
        "#for i in range(len(df_ds.items())):\n",
        "\n",
        "\n",
        "for t_k, t_v in df_ds.items():\n",
        "            #if t_k in list(df_ds.keys()) and :                \n",
        "                train_ltr.extend(t_v[0]) \n",
        " \n",
        "                for i in range(len(t_v[0])):\n",
        "                    y_train3.append(t_k)\n",
        "                test_ltr.extend(t_v[1]) \n",
        "                for i in range(len(t_v[1])):\n",
        "                    y_test3.append(t_k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train3, y_test3, train_ltr, test_ltr = np.asarray(y_train3), np.asarray(y_test3), np.asarray(train_ltr), np.asarray(test_ltr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(180, 28, 28)"
            ]
          },
          "execution_count": 384,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ltr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(180, 95, 180, 95)"
            ]
          },
          "execution_count": 385,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(y_train3), len(y_test3), len(train_ltr), len(test_ltr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 386,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_ltr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Here we represent the categorical values with numerical codes.\n",
        "\n",
        "y_train_code, y_train_uniques=pd.factorize(y_train3)\n",
        "y_test_code, y_test_uniques = pd.factorize(y_test3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array(['A', 'B', 'C', 'D', 'E'], dtype=object),\n",
              " array(['A', 'B', 'C', 'D', 'E'], dtype=object))"
            ]
          },
          "execution_count": 390,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#sanity check to see the items encoded. \n",
        "y_train_uniques, y_test_uniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "       4, 4, 4, 4], dtype=int64)"
            ]
          },
          "execution_count": 391,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# see the codes\n",
        "y_train_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QMxJtnSpD74",
        "outputId": "2631a148-2ab4-4609-c683-5bc987d39159"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((60000, 28, 28), (180, 28, 28), (95, 28, 28), (180,), (95,), (60000,))"
            ]
          },
          "execution_count": 393,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we need to visualize the shapes of the datasets going into the model\n",
        "# It should be: number of instances, image height, image width\n",
        "\n",
        "x_train.shape, train_ltr.shape, test_ltr.shape, y_train_code.shape, y_test_code.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {},
      "outputs": [],
      "source": [
        "#new_train = np.vstack((x_train, train_ltr))\n",
        "#new_test = np.vstack((x_test, test_ltr))\n",
        "#new_y_train=np.concatenate((y_train,y_train_code))\n",
        "#new_y_test=np.concatenate((y_test,y_test_code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((60180, 28, 28), (10095, 28, 28), (60180,), (10095,))"
            ]
          },
          "execution_count": 323,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#new_train.shape, new_test.shape, new_y_train.shape, new_y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We set important model fit hyperparameters here. \n",
        "- Kernel size of 7 is farely large but we will see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {
        "id": "bg8CCxB4o7I9"
      },
      "outputs": [],
      "source": [
        "now = datetime.datetime.now\n",
        "batch_size = 128\n",
        "num_classes = len(np.unique(y_train))\n",
        "epochs = 15\n",
        "img_rows, img_cols = 28, 28\n",
        "filters = 64\n",
        "pool_size = 2\n",
        "kernel_size = 7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {
        "id": "VhCSpsY2o7ls"
      },
      "outputs": [],
      "source": [
        "if K.image_data_format() == 'channels_first':\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESFAW0y6U5uc"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "metadata": {
        "id": "cfE4dnhlpD0O"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train, test, num_classes):\n",
        "    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n",
        "    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "    print('x_train shape:', x_train.shape)\n",
        "    print(x_train.shape[0], 'train samples')\n",
        "    print(x_test.shape[0], 'test samples')\n",
        "    # convert class vectors to binary class matrices\n",
        "    y_train = tf.keras.utils.to_categorical(train[1], num_classes)\n",
        "    y_test = tf.keras.utils.to_categorical(test[1], num_classes)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adadelta',\n",
        "                  metrics=['accuracy'])\n",
        "    t = now()\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              verbose=1,\n",
        "              validation_data=(x_test, y_test))\n",
        "    print('Training time: %s' % (now() - t))\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print('Test score:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l9g5e0ILaV0"
      },
      "source": [
        "## About the model.  \n",
        "- The 1st layer uses the 64 7 X 7 filters and takes input shape 28 X 28.  \n",
        "- The second layer uses the same number of filters and size. \n",
        "- We apply maxpooling after the second layer.  \n",
        "- standard 'relu' activation was selected. \n",
        "\n",
        "**output layer**  \n",
        "- These layers are flattened into a dense layer with 128 neurons.  \n",
        "- The output layer uses 'softmax' activation and outputs 5 classes contained in the dataset.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4w0WDTt6K_Z"
      },
      "source": [
        "### Train on all 10 digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "metadata": {
        "id": "KPr6KWKqpdUH"
      },
      "outputs": [],
      "source": [
        "feature_layers = [\n",
        "    Conv2D(filters, kernel_size,\n",
        "           padding='valid',\n",
        "           input_shape=input_shape),\n",
        "    Activation('relu'),\n",
        "    Conv2D(filters, kernel_size),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D(pool_size=pool_size),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "]\n",
        "\n",
        "classification_layers = [\n",
        "    Dense(128),\n",
        "    Activation('relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes),\n",
        "    Activation('softmax')\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 363,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "5AeSLAQwpgw_",
        "outputId": "2b9bd9dc-8dc1-4db1-f9b2-b56f954a7ffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Epoch 1/35\n",
            "469/469 [==============================] - 63s 135ms/step - loss: 2.2839 - accuracy: 0.1570 - val_loss: 2.2472 - val_accuracy: 0.4084\n",
            "Epoch 2/35\n",
            "469/469 [==============================] - 61s 129ms/step - loss: 2.2287 - accuracy: 0.2726 - val_loss: 2.1718 - val_accuracy: 0.5654\n",
            "Epoch 3/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 2.1499 - accuracy: 0.3661 - val_loss: 2.0563 - val_accuracy: 0.6316\n",
            "Epoch 4/35\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 2.0291 - accuracy: 0.4344 - val_loss: 1.8836 - val_accuracy: 0.6901\n",
            "Epoch 5/35\n",
            "469/469 [==============================] - 62s 131ms/step - loss: 1.8657 - accuracy: 0.4860 - val_loss: 1.6554 - val_accuracy: 0.7440\n",
            "Epoch 6/35\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 1.6765 - accuracy: 0.5301 - val_loss: 1.4072 - val_accuracy: 0.7769\n",
            "Epoch 7/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 1.4905 - accuracy: 0.5716 - val_loss: 1.1851 - val_accuracy: 0.7972\n",
            "Epoch 8/35\n",
            "469/469 [==============================] - 61s 131ms/step - loss: 1.3331 - accuracy: 0.6075 - val_loss: 1.0102 - val_accuracy: 0.8156\n",
            "Epoch 9/35\n",
            "469/469 [==============================] - 62s 133ms/step - loss: 1.2079 - accuracy: 0.6365 - val_loss: 0.8805 - val_accuracy: 0.8256\n",
            "Epoch 10/35\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 1.1060 - accuracy: 0.6620 - val_loss: 0.7817 - val_accuracy: 0.8339\n",
            "Epoch 11/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 1.0262 - accuracy: 0.6831 - val_loss: 0.7072 - val_accuracy: 0.8417\n",
            "Epoch 12/35\n",
            "469/469 [==============================] - 61s 131ms/step - loss: 0.9618 - accuracy: 0.7021 - val_loss: 0.6496 - val_accuracy: 0.8487\n",
            "Epoch 13/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 0.9103 - accuracy: 0.7167 - val_loss: 0.6043 - val_accuracy: 0.8557\n",
            "Epoch 14/35\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 0.8616 - accuracy: 0.7311 - val_loss: 0.5658 - val_accuracy: 0.8612\n",
            "Epoch 15/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 0.8216 - accuracy: 0.7419 - val_loss: 0.5344 - val_accuracy: 0.8679\n",
            "Epoch 16/35\n",
            "469/469 [==============================] - 62s 131ms/step - loss: 0.7886 - accuracy: 0.7535 - val_loss: 0.5088 - val_accuracy: 0.8726\n",
            "Epoch 17/35\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 0.7592 - accuracy: 0.7632 - val_loss: 0.4855 - val_accuracy: 0.8777\n",
            "Epoch 18/35\n",
            "469/469 [==============================] - 62s 131ms/step - loss: 0.7331 - accuracy: 0.7724 - val_loss: 0.4647 - val_accuracy: 0.8802\n",
            "Epoch 19/35\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 0.7024 - accuracy: 0.7811 - val_loss: 0.4462 - val_accuracy: 0.8841\n",
            "Epoch 20/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 0.6846 - accuracy: 0.7876 - val_loss: 0.4304 - val_accuracy: 0.8880\n",
            "Epoch 21/35\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 0.6625 - accuracy: 0.7948 - val_loss: 0.4160 - val_accuracy: 0.8917\n",
            "Epoch 22/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 0.6418 - accuracy: 0.8023 - val_loss: 0.4017 - val_accuracy: 0.8959\n",
            "Epoch 23/35\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 0.6265 - accuracy: 0.8061 - val_loss: 0.3901 - val_accuracy: 0.8993\n",
            "Epoch 24/35\n",
            "469/469 [==============================] - 63s 133ms/step - loss: 0.6094 - accuracy: 0.8130 - val_loss: 0.3784 - val_accuracy: 0.9011\n",
            "Epoch 25/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 0.5981 - accuracy: 0.8145 - val_loss: 0.3690 - val_accuracy: 0.9037\n",
            "Epoch 26/35\n",
            "469/469 [==============================] - 63s 133ms/step - loss: 0.5789 - accuracy: 0.8227 - val_loss: 0.3588 - val_accuracy: 0.9053\n",
            "Epoch 27/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 0.5694 - accuracy: 0.8249 - val_loss: 0.3507 - val_accuracy: 0.9069\n",
            "Epoch 28/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 0.5573 - accuracy: 0.8286 - val_loss: 0.3419 - val_accuracy: 0.9093\n",
            "Epoch 29/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 0.5451 - accuracy: 0.8328 - val_loss: 0.3342 - val_accuracy: 0.9115\n",
            "Epoch 30/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 0.5328 - accuracy: 0.8379 - val_loss: 0.3268 - val_accuracy: 0.9132\n",
            "Epoch 31/35\n",
            "469/469 [==============================] - 61s 131ms/step - loss: 0.5212 - accuracy: 0.8406 - val_loss: 0.3191 - val_accuracy: 0.9152\n",
            "Epoch 32/35\n",
            "469/469 [==============================] - 62s 133ms/step - loss: 0.5092 - accuracy: 0.8464 - val_loss: 0.3118 - val_accuracy: 0.9168\n",
            "Epoch 33/35\n",
            "469/469 [==============================] - 62s 131ms/step - loss: 0.5025 - accuracy: 0.8473 - val_loss: 0.3055 - val_accuracy: 0.9178\n",
            "Epoch 34/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 0.4908 - accuracy: 0.8498 - val_loss: 0.2989 - val_accuracy: 0.9198\n",
            "Epoch 35/35\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 0.4821 - accuracy: 0.8544 - val_loss: 0.2929 - val_accuracy: 0.9212\n",
            "Training time: 0:35:58.876706\n",
            "Test score: 0.29291051626205444\n",
            "Test accuracy: 0.9211999773979187\n"
          ]
        }
      ],
      "source": [
        "# create complete model\n",
        "model = Sequential(feature_layers + classification_layers)\n",
        "\n",
        "# train model for 5-digit classification [0..4]\n",
        "train_model(model, [x_train, y_train],[x_test, y_test], num_classes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((180, 28, 28), (95, 28, 28), (180,), (95,))"
            ]
          },
          "execution_count": 364,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ltr.shape, test_ltr.shape, y_train_code.shape, y_test_code.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 365,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 365,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes= len(np.unique(y_train_code))\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output layer.  \n",
        "- The layer has been to changed to 5 since we only have A, B, C, D, E in the new dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 366,
      "metadata": {},
      "outputs": [],
      "source": [
        "classification_layers1 = [\n",
        "    Dense(128),\n",
        "    Activation('relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(5),\n",
        "    Activation('softmax')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 367,
      "metadata": {
        "id": "IYKICfy-pgoF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (180, 28, 28, 1)\n",
            "180 train samples\n",
            "95 test samples\n",
            "Epoch 1/35\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 2.3188 - accuracy: 0.2222 - val_loss: 1.6959 - val_accuracy: 0.2526\n",
            "Epoch 2/35\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 2.4216 - accuracy: 0.2222 - val_loss: 1.6890 - val_accuracy: 0.2632\n",
            "Epoch 3/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.3918 - accuracy: 0.2389 - val_loss: 1.6817 - val_accuracy: 0.2526\n",
            "Epoch 4/35\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 2.3234 - accuracy: 0.1667 - val_loss: 1.6755 - val_accuracy: 0.2526\n",
            "Epoch 5/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.4153 - accuracy: 0.1500 - val_loss: 1.6698 - val_accuracy: 0.2421\n",
            "Epoch 6/35\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 2.4230 - accuracy: 0.2167 - val_loss: 1.6635 - val_accuracy: 0.2526\n",
            "Epoch 7/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.1568 - accuracy: 0.2389 - val_loss: 1.6580 - val_accuracy: 0.2526\n",
            "Epoch 8/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.1349 - accuracy: 0.2444 - val_loss: 1.6540 - val_accuracy: 0.2526\n",
            "Epoch 9/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.3118 - accuracy: 0.1889 - val_loss: 1.6493 - val_accuracy: 0.2526\n",
            "Epoch 10/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.3030 - accuracy: 0.1778 - val_loss: 1.6445 - val_accuracy: 0.2632\n",
            "Epoch 11/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.1542 - accuracy: 0.2500 - val_loss: 1.6403 - val_accuracy: 0.2632\n",
            "Epoch 12/35\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 2.3009 - accuracy: 0.2111 - val_loss: 1.6371 - val_accuracy: 0.2632\n",
            "Epoch 13/35\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 2.3757 - accuracy: 0.2056 - val_loss: 1.6324 - val_accuracy: 0.2632\n",
            "Epoch 14/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.2660 - accuracy: 0.2000 - val_loss: 1.6293 - val_accuracy: 0.2632\n",
            "Epoch 15/35\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 2.1745 - accuracy: 0.2278 - val_loss: 1.6249 - val_accuracy: 0.2632\n",
            "Epoch 16/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.3728 - accuracy: 0.2111 - val_loss: 1.6203 - val_accuracy: 0.2632\n",
            "Epoch 17/35\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 2.0335 - accuracy: 0.2778 - val_loss: 1.6166 - val_accuracy: 0.2632\n",
            "Epoch 18/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.0431 - accuracy: 0.3000 - val_loss: 1.6140 - val_accuracy: 0.2632\n",
            "Epoch 19/35\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 2.2704 - accuracy: 0.2611 - val_loss: 1.6111 - val_accuracy: 0.2632\n",
            "Epoch 20/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.3697 - accuracy: 0.1722 - val_loss: 1.6073 - val_accuracy: 0.2632\n",
            "Epoch 21/35\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 2.3310 - accuracy: 0.1944 - val_loss: 1.6045 - val_accuracy: 0.2526\n",
            "Epoch 22/35\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 2.3137 - accuracy: 0.2278 - val_loss: 1.6002 - val_accuracy: 0.2632\n",
            "Epoch 23/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.1582 - accuracy: 0.2444 - val_loss: 1.5974 - val_accuracy: 0.2737\n",
            "Epoch 24/35\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 2.1727 - accuracy: 0.2111 - val_loss: 1.5948 - val_accuracy: 0.2737\n",
            "Epoch 25/35\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 2.1608 - accuracy: 0.3000 - val_loss: 1.5920 - val_accuracy: 0.2737\n",
            "Epoch 26/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.2647 - accuracy: 0.1889 - val_loss: 1.5894 - val_accuracy: 0.2737\n",
            "Epoch 27/35\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 2.1938 - accuracy: 0.2500 - val_loss: 1.5871 - val_accuracy: 0.2632\n",
            "Epoch 28/35\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 2.3124 - accuracy: 0.2167 - val_loss: 1.5850 - val_accuracy: 0.2737\n",
            "Epoch 29/35\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 2.1748 - accuracy: 0.2556 - val_loss: 1.5826 - val_accuracy: 0.2737\n",
            "Epoch 30/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.1832 - accuracy: 0.1722 - val_loss: 1.5794 - val_accuracy: 0.2842\n",
            "Epoch 31/35\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 2.2215 - accuracy: 0.2056 - val_loss: 1.5776 - val_accuracy: 0.2842\n",
            "Epoch 32/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.0095 - accuracy: 0.2500 - val_loss: 1.5752 - val_accuracy: 0.2842\n",
            "Epoch 33/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.1223 - accuracy: 0.2333 - val_loss: 1.5737 - val_accuracy: 0.2842\n",
            "Epoch 34/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.2000 - accuracy: 0.2333 - val_loss: 1.5724 - val_accuracy: 0.2842\n",
            "Epoch 35/35\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 2.0780 - accuracy: 0.2722 - val_loss: 1.5709 - val_accuracy: 0.2842\n",
            "Training time: 0:00:03.642433\n",
            "Test score: 1.5709148645401\n",
            "Test accuracy: 0.28421053290367126\n"
          ]
        }
      ],
      "source": [
        "# freeze feature layers' weights and rebuild model\n",
        "model = Sequential(feature_layers + classification_layers1)\n",
        "#model1 = keras.Model(inputs=feature_layers.input, outputs=model)\n",
        "for l in feature_layers:\n",
        "    l.trainable = False\n",
        "\n",
        "# transfer: train dense layers for new classification task [5..9]\n",
        "train_model(model,\n",
        "            (train_ltr, y_train_code),\n",
        "            (test_ltr, y_test_code), num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation.  \n",
        "Looking the training result above (Test Accuracy of 28%), the model architecture performed poorly on this dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (180, 28, 28, 1)\n",
            "180 train samples\n",
            "95 test samples\n",
            "Epoch 1/35\n",
            "2/2 [==============================] - 1s 176ms/step - loss: 2.2208 - accuracy: 0.1833 - val_loss: 1.5674 - val_accuracy: 0.2842\n",
            "Epoch 2/35\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 2.1855 - accuracy: 0.2222 - val_loss: 1.5643 - val_accuracy: 0.2842\n",
            "Epoch 3/35\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 2.4249 - accuracy: 0.1667 - val_loss: 1.5617 - val_accuracy: 0.2842\n",
            "Epoch 4/35\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 2.0500 - accuracy: 0.2333 - val_loss: 1.5579 - val_accuracy: 0.2842\n",
            "Epoch 5/35\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 2.0957 - accuracy: 0.2167 - val_loss: 1.5559 - val_accuracy: 0.2842\n",
            "Epoch 6/35\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 2.2555 - accuracy: 0.1778 - val_loss: 1.5537 - val_accuracy: 0.2842\n",
            "Epoch 7/35\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 2.2606 - accuracy: 0.1833 - val_loss: 1.5502 - val_accuracy: 0.2842\n",
            "Epoch 8/35\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 2.1015 - accuracy: 0.2444 - val_loss: 1.5472 - val_accuracy: 0.3053\n",
            "Epoch 9/35\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 2.0140 - accuracy: 0.2889 - val_loss: 1.5443 - val_accuracy: 0.3158\n",
            "Epoch 10/35\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 2.0473 - accuracy: 0.2500 - val_loss: 1.5420 - val_accuracy: 0.3158\n",
            "Epoch 11/35\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 2.0482 - accuracy: 0.2222 - val_loss: 1.5397 - val_accuracy: 0.3263\n",
            "Epoch 12/35\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 2.2455 - accuracy: 0.1944 - val_loss: 1.5375 - val_accuracy: 0.3158\n",
            "Epoch 13/35\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 2.1524 - accuracy: 0.2278 - val_loss: 1.5350 - val_accuracy: 0.3579\n",
            "Epoch 14/35\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 2.0137 - accuracy: 0.2167 - val_loss: 1.5332 - val_accuracy: 0.3474\n",
            "Epoch 15/35\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 2.1269 - accuracy: 0.1833 - val_loss: 1.5310 - val_accuracy: 0.3474\n",
            "Epoch 16/35\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 2.0462 - accuracy: 0.2611 - val_loss: 1.5296 - val_accuracy: 0.3263\n",
            "Epoch 17/35\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 2.0013 - accuracy: 0.2333 - val_loss: 1.5280 - val_accuracy: 0.3474\n",
            "Epoch 18/35\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 2.0251 - accuracy: 0.2778 - val_loss: 1.5259 - val_accuracy: 0.3474\n",
            "Epoch 19/35\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 2.2186 - accuracy: 0.1611 - val_loss: 1.5245 - val_accuracy: 0.3474\n",
            "Epoch 20/35\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 2.0275 - accuracy: 0.2111 - val_loss: 1.5231 - val_accuracy: 0.3474\n",
            "Epoch 21/35\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 2.0603 - accuracy: 0.2333 - val_loss: 1.5212 - val_accuracy: 0.3579\n",
            "Epoch 22/35\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 1.9264 - accuracy: 0.2500 - val_loss: 1.5202 - val_accuracy: 0.3579\n",
            "Epoch 23/35\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 2.0289 - accuracy: 0.2167 - val_loss: 1.5190 - val_accuracy: 0.3579\n",
            "Epoch 24/35\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 2.0985 - accuracy: 0.2222 - val_loss: 1.5177 - val_accuracy: 0.3579\n",
            "Epoch 25/35\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 2.0705 - accuracy: 0.2500 - val_loss: 1.5163 - val_accuracy: 0.3474\n",
            "Epoch 26/35\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 2.0948 - accuracy: 0.2389 - val_loss: 1.5149 - val_accuracy: 0.3579\n",
            "Epoch 27/35\n",
            "2/2 [==============================] - 0s 92ms/step - loss: 1.9550 - accuracy: 0.2500 - val_loss: 1.5141 - val_accuracy: 0.3579\n",
            "Epoch 28/35\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 2.0278 - accuracy: 0.2444 - val_loss: 1.5130 - val_accuracy: 0.3579\n",
            "Epoch 29/35\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 1.9967 - accuracy: 0.2278 - val_loss: 1.5120 - val_accuracy: 0.3789\n",
            "Epoch 30/35\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 2.0782 - accuracy: 0.2444 - val_loss: 1.5102 - val_accuracy: 0.3579\n",
            "Epoch 31/35\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 1.7737 - accuracy: 0.2389 - val_loss: 1.5089 - val_accuracy: 0.3684\n",
            "Epoch 32/35\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 2.0479 - accuracy: 0.2222 - val_loss: 1.5083 - val_accuracy: 0.3579\n",
            "Epoch 33/35\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 1.9412 - accuracy: 0.2333 - val_loss: 1.5071 - val_accuracy: 0.3789\n",
            "Epoch 34/35\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 1.8847 - accuracy: 0.1944 - val_loss: 1.5058 - val_accuracy: 0.3579\n",
            "Epoch 35/35\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 2.0149 - accuracy: 0.2667 - val_loss: 1.5050 - val_accuracy: 0.3684\n",
            "Training time: 0:00:08.067619\n",
            "Test score: 1.5050079822540283\n",
            "Test accuracy: 0.3684210479259491\n"
          ]
        }
      ],
      "source": [
        "# unfreeze feature layers and rebuild model to see the difference\n",
        "model = Sequential(feature_layers + classification_layers1)\n",
        "#model1 = keras.Model(inputs=feature_layers.input, outputs=model)\n",
        "for l in feature_layers:\n",
        "    l.trainable = True\n",
        "\n",
        "# transfer: train dense layers for new classification task [5..9]\n",
        "train_model(model,\n",
        "            (train_ltr, y_train_code),\n",
        "            (test_ltr, y_test_code), num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- When unfroze the layers, there was still no significant improvement to the accuracy result.  \n",
        "- This suggests that the model architecture is not suitable for this dataset features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ml2_hw.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
